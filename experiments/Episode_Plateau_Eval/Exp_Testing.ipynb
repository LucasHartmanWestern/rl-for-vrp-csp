{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d74d53a-b29c-4a03-9db8-ff0dee7a3821",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d3f829-f054-4020-8a5b-c4756529698b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mplcursors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/github/rl-for-vrp-csp/evaluation.py:6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdata_loader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save_to_csv, read_csv_data\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmplcursors\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate\u001b[39m(ev_info, metrics, seed, date, verbose, purpose, num_episodes, base_path, append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m purpose \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mplcursors'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../'))\n",
    "\n",
    "from evaluation import *\n",
    "from data_loader import *\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d7496-1478-4534-8755-703e0f0c2202",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e6f3f-546b-4bb4-b852-6b66ebdfaad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#experiments = [7, 16, 45]\n",
    "#experiments = [0, 1000, 1001, 1002, 1003, 1004]\n",
    "#experiments = [7]#, 16]\n",
    "#experiments = [1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016] # Lower cars\n",
    "experiments = [1014] # Communal reward\n",
    "#experiments = [1017, 1018, 1019, 1020] # Increase cars to charger ratio\n",
    "#experiments = [477, 478, 479] # Cumulative reward experiments\n",
    "experiments = [1021, 1022, 1023, 1024] # Increase cars to charger ratio and use communal reward\n",
    "#experiments = [1025, 1026] # 2500 eps using 5 zones and using only zones 2 and 4 (same as 1011 otherwise)\n",
    "\n",
    "exp_agent_data = []\n",
    "\n",
    "for exp_num in experiments:\n",
    "    config_fname = f'../Exp_{exp_num}/config.yaml'\n",
    "    \n",
    "    c = load_config_file(config_fname)\n",
    "    nn_c = c['nn_hyperparameters']\n",
    "    federated_c = c['federated_learning_settings']\n",
    "    algo_c = c['algorithm_settings']\n",
    "    env_c = c['environment_settings']\n",
    "    eval_c = c['eval_config']\n",
    "    \n",
    "    verbose = eval_c['verbose']\n",
    "    \n",
    "    if eval_c['fixed_attributes'] != [0, 1] and eval_c['fixed_attributes'] != [1, 0] and eval_c['fixed_attributes'] != [0.5, 0.5]:\n",
    "        attr_label = 'learned'\n",
    "    else:\n",
    "        fixed_attributes = eval_c['fixed_attributes']\n",
    "        attr_label = f'{fixed_attributes[0]}_{fixed_attributes[1]}'\n",
    "    \n",
    "    current_datetime = datetime.now()\n",
    "    date = current_datetime.strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    seed = env_c['seed']\n",
    "\n",
    "    algorithm_dm = algo_c['algorithm']\n",
    "    \n",
    "    def load_from_json_with_error_handling(filepath):\n",
    "        try:\n",
    "            return read_csv_data(filepath, columns=['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'car_model', 'reward'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "            return None  # Handle the error and return None or an empty object\n",
    "    \n",
    "    d_base = f\"../../../../../storage_1/metrics/Exp_{exp_num}\"\n",
    "    \n",
    "    if not os.path.exists(d_base):\n",
    "        d_base = f\"../metrics/Exp_{exp_num}\"\n",
    "            \n",
    "    base_path = f\"{d_base}/train/metrics\"\n",
    "\n",
    "    print(f'Loading {base_path}_agent_metrics.csv')\n",
    "    agent_data = load_from_json_with_error_handling(f'{base_path}_agent_metrics.csv')\n",
    "    agent_data['seed'] = seed\n",
    "    agent_data['exp_num'] = exp_num\n",
    "    agent_data['algorithm'] = algorithm_dm\n",
    "    agent_data['season'] = env_c['season']\n",
    "\n",
    "    exp_agent_data.append(agent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc565e-b299-473d-a720-59db327a83cc",
   "metadata": {},
   "source": [
    "### Modify reward data to be cumulative averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3ca464-cdd7-4e84-8c92-251a9adc0e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to DataFrame for easier manipulation\n",
    "df = pd.concat(exp_agent_data, ignore_index=True)\n",
    "\n",
    "# Scale timestep rewards by timestep index (uncomment this for reward v2)\n",
    "#df['reward'] = df['reward'] / (df['timestep'] + 1)\n",
    "\n",
    "# Only show select timesteps\n",
    "#df = df[df['timestep'] >= 2]\n",
    "\n",
    "# Create a new DataFrame that just has the cumulative reward per episode, zone, aggregation, agent_index, and seed\n",
    "cumulative_reward_df = df.groupby(['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season'])['reward'].sum().reset_index()\n",
    "\n",
    "# Rename the 'reward' column to 'cumulative_reward' for clarity\n",
    "cumulative_reward_df.rename(columns={'reward': 'cumulative_reward'}, inplace=True)\n",
    "\n",
    "# Get recalculated episodes using (aggregation number * episodes per aggregation) + episode number\n",
    "cumulative_reward_df['episode'] = cumulative_reward_df['aggregation'] * nn_c['num_episodes'] + cumulative_reward_df['episode']\n",
    "\n",
    "# **Sort the DataFrame to ensure correct order for expanding mean**\n",
    "cumulative_reward_df.sort_values(['zone', 'agent_index', 'seed', 'episode'], inplace=True)\n",
    "\n",
    "# **Calculate cumulative average reward per episode, zone, agent_index, and seed**\n",
    "cumulative_reward_df['cumulative_avg_reward'] = cumulative_reward_df.groupby(\n",
    "    ['zone', 'agent_index', 'seed']\n",
    ")['cumulative_reward'].expanding().mean().reset_index(level=[0, 1, 2], drop=True)\n",
    "\n",
    "\n",
    "# Calculate cumulative average reward per episode and zone\n",
    "cumulative_avg_reward_by_zone = cumulative_reward_df.groupby(['episode', 'zone', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_zone['cumulative_reward'] = cumulative_avg_reward_by_zone.groupby(['zone', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and experiment number\n",
    "cumulative_avg_reward_by_exp_num = cumulative_reward_df.groupby(['episode', 'exp_num', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_exp_num['cumulative_reward'] = cumulative_avg_reward_by_exp_num.groupby(['exp_num', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and algorithm\n",
    "cumulative_avg_reward_by_algorithm = cumulative_reward_df.groupby(['episode', 'algorithm', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_algorithm['cumulative_reward'] = cumulative_avg_reward_by_algorithm.groupby(['algorithm', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and algorithm and season\n",
    "cumulative_avg_reward_by_algorithm_season = cumulative_reward_df.groupby(\n",
    "    ['episode', 'algorithm', 'season', 'seed']\n",
    ")['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_algorithm_season = cumulative_avg_reward_by_algorithm_season.sort_values(\n",
    "    ['algorithm', 'season', 'seed', 'episode']\n",
    ")\n",
    "cumulative_avg_reward_by_algorithm_season['cumulative_avg_reward'] = cumulative_avg_reward_by_algorithm_season.groupby(\n",
    "    ['algorithm', 'season', 'seed']\n",
    ")['cumulative_reward'].transform(lambda x: x.expanding().mean())\n",
    "\n",
    "# Calculate cumulative average reward per episode and season\n",
    "cumulative_avg_reward_by_season = cumulative_reward_df.groupby(['episode', 'season', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_season['cumulative_reward'] = cumulative_avg_reward_by_season.groupby(['season', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "### PLOT DATA\n",
    "\n",
    "# **Prepare data for plotting by aggregating over seeds and agent indices**\n",
    "plot_data = cumulative_reward_df.groupby(['episode', 'zone']).agg({\n",
    "    'cumulative_avg_reward': ['min', 'max', 'mean']\n",
    "}).reset_index()\n",
    "plot_data.columns = ['episode', 'zone', 'min_reward', 'max_reward', 'mean_reward']\n",
    "\n",
    "# Plot the cloud plot for each zone\n",
    "plt.figure(figsize=(8, 6))\n",
    "for zone in plot_data['zone'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    zone_data = plot_data[plot_data['zone'] == zone]\n",
    "    \n",
    "    plt.fill_between(\n",
    "        zone_data['episode'], \n",
    "        zone_data['min_reward'], \n",
    "        zone_data['max_reward'], \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        zone_data['episode'], \n",
    "        zone_data['mean_reward'], \n",
    "        label=f'Zone {zone} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title('Reward by Zone and Agent Index')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "### OLD PLOT DATA\n",
    "\n",
    "# Plot the cloud plot for each zone\n",
    "plt.figure(figsize=(8, 6))\n",
    "for zone in df['zone'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    zone_data = cumulative_avg_reward_by_zone.loc[cumulative_avg_reward_by_zone['zone'] == zone]\n",
    "    min_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Zone {zone} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Zone and Seed')\n",
    "#plt.title(f'Version 2 Reward Function')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the cloud plot for each experiment\n",
    "plt.figure(figsize=(8, 6))\n",
    "for exp_num in df['exp_num'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    exp_data_data = cumulative_avg_reward_by_exp_num.loc[cumulative_avg_reward_by_exp_num['exp_num'] == exp_num]\n",
    "    mean_cumulative_avg_reward = exp_data_data.groupby('episode')['cumulative_reward'].mean()\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Experiment {exp_num} Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Experiment Number and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the cloud plot for each algorithm\n",
    "plt.figure(figsize=(8, 6))\n",
    "for algo in df['algorithm'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    algo_data = cumulative_avg_reward_by_algorithm.loc[cumulative_avg_reward_by_algorithm['algorithm'] == algo]\n",
    "    min_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Algorithm {algo} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Algorithm and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Increased size for better visibility\n",
    "for algo in cumulative_avg_reward_by_algorithm_season['algorithm'].unique():\n",
    "    for season in cumulative_avg_reward_by_algorithm_season['season'].unique():\n",
    "        # Filter the data for the current algorithm and season\n",
    "        data = cumulative_avg_reward_by_algorithm_season.loc[\n",
    "            (cumulative_avg_reward_by_algorithm_season['algorithm'] == algo) &\n",
    "            (cumulative_avg_reward_by_algorithm_season['season'] == season)\n",
    "        ]\n",
    "        \n",
    "        if data.empty:\n",
    "            continue  # Skip if no data for this combination\n",
    "        \n",
    "        # Calculate min, max, and mean cumulative rewards per episode\n",
    "        min_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].min()\n",
    "        max_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].max()\n",
    "        mean_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].mean()\n",
    "        \n",
    "        # Plot the filled area between min and max\n",
    "        plt.fill_between(\n",
    "            min_cumulative_avg_reward.index, \n",
    "            min_cumulative_avg_reward.values, \n",
    "            max_cumulative_avg_reward.values, \n",
    "            alpha=0.3\n",
    "        )\n",
    "        \n",
    "        # Plot the mean cumulative average reward\n",
    "        plt.plot(\n",
    "            mean_cumulative_avg_reward.index, \n",
    "            mean_cumulative_avg_reward.values, \n",
    "            label=f'Algorithm {algo} Season {season} Average Reward'\n",
    "        )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(\n",
    "    f'Aggs. {federated_c[\"aggregation_count\"]} - '\n",
    "    f'Eps. per agg. {nn_c[\"num_episodes\"]} - '\n",
    "    f'Cumulative Average Reward per Episode by Algorithm and Season'\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to accommodate legend\n",
    "plt.show()\n",
    "\n",
    "# Plot the cloud plot for each season\n",
    "plt.figure(figsize=(8, 6))\n",
    "for season in df['season'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    season_data = cumulative_avg_reward_by_season.loc[cumulative_avg_reward_by_season['season'] == season]\n",
    "    min_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Season {season} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Season and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
