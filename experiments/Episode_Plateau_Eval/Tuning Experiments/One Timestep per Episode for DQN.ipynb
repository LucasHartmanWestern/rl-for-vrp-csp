{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d74d53a-b29c-4a03-9db8-ff0dee7a3821",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8d3f829-f054-4020-8a5b-c4756529698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../../../'))\n",
    "\n",
    "from evaluation import *\n",
    "from data_loader import *\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d7496-1478-4534-8755-703e0f0c2202",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113e6f3f-546b-4bb4-b852-6b66ebdfaad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../metrics/Exp_120/train/metrics_agent_metrics.csv\n",
      "File ../metrics/Exp_120/train/metrics_agent_metrics.csv not found.\n",
      "Loading ../metrics/Exp_121/train/metrics_agent_metrics.csv\n",
      "File ../metrics/Exp_121/train/metrics_agent_metrics.csv not found.\n",
      "Loading ../metrics/Exp_122/train/metrics_agent_metrics.csv\n",
      "File ../metrics/Exp_122/train/metrics_agent_metrics.csv not found.\n"
     ]
    }
   ],
   "source": [
    "experiments = [120, 121, 122]\n",
    "\n",
    "exp_agent_data = []\n",
    "\n",
    "for exp_num in experiments:\n",
    "    config_fname = f'../../Exp_{exp_num}/config.yaml'\n",
    "    \n",
    "    c = load_config_file(config_fname)\n",
    "    nn_c = c['nn_hyperparameters']\n",
    "    federated_c = c['federated_learning_settings']\n",
    "    algo_c = c['algorithm_settings']\n",
    "    env_c = c['environment_settings']\n",
    "    eval_c = c['eval_config']\n",
    "    \n",
    "    verbose = eval_c['verbose']\n",
    "    \n",
    "    if eval_c['fixed_attributes'] != [0, 1] and eval_c['fixed_attributes'] != [1, 0] and eval_c['fixed_attributes'] != [0.5, 0.5]:\n",
    "        attr_label = 'learned'\n",
    "    else:\n",
    "        fixed_attributes = eval_c['fixed_attributes']\n",
    "        attr_label = f'{fixed_attributes[0]}_{fixed_attributes[1]}'\n",
    "    \n",
    "    current_datetime = datetime.now()\n",
    "    date = current_datetime.strftime('%Y-%m-%d_%H-%M')\n",
    "\n",
    "    ev_info = []\n",
    "\n",
    "    seed = env_c['seed']\n",
    "    \n",
    "    # Assign seed\n",
    "    random.seed(seed)\n",
    "    # Creating and seeding a random generaton from Numpy\n",
    "    rng = np.random.default_rng(seed)\n",
    "    \n",
    "    # Generate a random model index for each agent\n",
    "    model_indices = np.array([random.randrange(3) for agent in range(env_c['num_of_cars'])], dtype=int)\n",
    "\n",
    "    # Use the indices to select the model type and corresponding configurations\n",
    "    model_type = np.array([env_c['models'][index] for index in model_indices], dtype=str)\n",
    "    usage_per_hour = np.array([env_c['usage_per_hour'][index] for index in model_indices], dtype=int)\n",
    "    max_charge = np.array([env_c['max_charge'][index] for index in model_indices], dtype=int)\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Random charge between 0.5-x%, where x scales between 1-25% as sessions continue\n",
    "    starting_charge = env_c['starting_charge'] + 2000*(rng.random(env_c['num_of_cars'])-0.5)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    # Define a structured array\n",
    "    dtypes = [('starting_charge', float),\n",
    "                ('max_charge', int),\n",
    "                ('usage_per_hour', int),\n",
    "                ('model_type', 'U50'),  # Adjust string length as needed\n",
    "                ('model_indices', int)]\n",
    "    info = np.zeros(env_c['num_of_cars'], dtype=dtypes)\n",
    "\n",
    "    # Assign values\n",
    "    info['starting_charge'] = starting_charge\n",
    "    info['max_charge'] = max_charge\n",
    "    info['usage_per_hour'] = usage_per_hour\n",
    "    info['model_type'] = model_type\n",
    "    info['model_indices'] = model_indices\n",
    "\n",
    "    ev_info.append(info)\n",
    "\n",
    "    algorithm_dm = algo_c['algorithm']\n",
    "    \n",
    "    def load_from_json_with_error_handling(filepath):\n",
    "        try:\n",
    "            return read_csv_data(filepath, columns=['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'car_model', 'reward'])\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "            return None  # Handle the error and return None or an empty object\n",
    "    \n",
    "    \n",
    "    d_base = f\"../../../../../../storage_1/metrics/Exp_{exp_num}\"\n",
    "    \n",
    "    if not os.path.exists(d_base):\n",
    "        d_base = f\"../metrics/Exp_{exp_num}\"\n",
    "            \n",
    "    base_path = f\"{d_base}/train/metrics\"\n",
    "\n",
    "    print(f'Loading {base_path}_agent_metrics.csv')\n",
    "    agent_data = load_from_json_with_error_handling(f'{base_path}_agent_metrics.csv')\n",
    "    agent_data['seed'] = seed\n",
    "    agent_data['exp_num'] = exp_num\n",
    "    agent_data['algorithm'] = algorithm_dm\n",
    "    agent_data['season'] = env_c['season']\n",
    "\n",
    "    exp_agent_data.append(agent_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fc565e-b299-473d-a720-59db327a83cc",
   "metadata": {},
   "source": [
    "### Modify reward data to be cumulative averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c3ca464-cdd7-4e84-8c92-251a9adc0e5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'episode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(exp_agent_data, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Scale timestep rewards by timestep index (uncomment this for reward v2)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#df['reward'] = df['reward'] / (df['timestep'] + 1)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create a new DataFrame that just has the cumulative reward per episode, zone, aggregation, agent_index, and seed\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m cumulative_reward_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maggregation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexp_num\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malgorithm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mseason\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Rename the 'reward' column to 'cumulative_reward' for clarity\u001b[39;00m\n\u001b[1;32m     14\u001b[0m cumulative_reward_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcumulative_reward\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:8252\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   8249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   8250\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[0;32m-> 8252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   8253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8255\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8258\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   8262\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:931\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    937\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    939\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/groupby/grouper.py:985\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m    983\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 985\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    986\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    988\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'episode'"
     ]
    }
   ],
   "source": [
    "# Convert data to DataFrame for easier manipulation\n",
    "df = pd.concat(exp_agent_data, ignore_index=True)\n",
    "\n",
    "# Scale timestep rewards by timestep index (uncomment this for reward v2)\n",
    "#df['reward'] = df['reward'] / (df['timestep'] + 1)\n",
    "\n",
    "# Only show a single timestep\n",
    "#df = df[df['timestep'] == 0]\n",
    "\n",
    "# Create a new DataFrame that just has the cumulative reward per episode, zone, aggregation, agent_index, and seed\n",
    "cumulative_reward_df = df.groupby(['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season'])['reward'].sum().reset_index()\n",
    "\n",
    "# Rename the 'reward' column to 'cumulative_reward' for clarity\n",
    "cumulative_reward_df.rename(columns={'reward': 'cumulative_reward'}, inplace=True)\n",
    "\n",
    "# Get recalculated episodes using (aggregation number * episodes per aggregation) + episode number\n",
    "cumulative_reward_df['episode'] = cumulative_reward_df['aggregation'] * nn_c['num_episodes'] + cumulative_reward_df['episode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1255ef0c-2d51-4a9c-8699-19a4c880f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cumulative average reward per episode and zone\n",
    "cumulative_avg_reward_by_zone = cumulative_reward_df.groupby(['episode', 'zone', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_zone['cumulative_reward'] = cumulative_avg_reward_by_zone.groupby(['zone', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and experiment number\n",
    "cumulative_avg_reward_by_exp_num = cumulative_reward_df.groupby(['episode', 'exp_num', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_exp_num['cumulative_reward'] = cumulative_avg_reward_by_exp_num.groupby(['exp_num', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and algorithm\n",
    "cumulative_avg_reward_by_algorithm = cumulative_reward_df.groupby(['episode', 'algorithm', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_algorithm['cumulative_reward'] = cumulative_avg_reward_by_algorithm.groupby(['algorithm', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)\n",
    "\n",
    "# Calculate cumulative average reward per episode and algorithm and season\n",
    "cumulative_avg_reward_by_algorithm_season = cumulative_reward_df.groupby(\n",
    "    ['episode', 'algorithm', 'season', 'seed']\n",
    ")['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_algorithm_season = cumulative_avg_reward_by_algorithm_season.sort_values(\n",
    "    ['algorithm', 'season', 'seed', 'episode']\n",
    ")\n",
    "cumulative_avg_reward_by_algorithm_season['cumulative_avg_reward'] = cumulative_avg_reward_by_algorithm_season.groupby(\n",
    "    ['algorithm', 'season', 'seed']\n",
    ")['cumulative_reward'].transform(lambda x: x.expanding().mean())\n",
    "\n",
    "# Calculate cumulative average reward per episode and season\n",
    "cumulative_avg_reward_by_season = cumulative_reward_df.groupby(['episode', 'season', 'seed'])['cumulative_reward'].mean().reset_index()\n",
    "cumulative_avg_reward_by_season['cumulative_reward'] = cumulative_avg_reward_by_season.groupby(['season', 'seed'])['cumulative_reward'].expanding().mean().reset_index(level=[0, 1], drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b62846a-00cd-4293-9c3d-33f5771060b8",
   "metadata": {},
   "source": [
    "### Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8ce559-3dcc-4ad7-a851-a06007d404a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the cloud plot for each zone\n",
    "plt.figure(figsize=(8, 6))\n",
    "for zone in df['zone'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    zone_data = cumulative_avg_reward_by_zone.loc[cumulative_avg_reward_by_zone['zone'] == zone]\n",
    "    min_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = zone_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Zone {zone} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "#plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Zone and Seed')\n",
    "plt.title(f'Version 2 Reward Function')\n",
    "\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the cloud plot for each experiment\n",
    "plt.figure(figsize=(8, 6))\n",
    "for exp_num in df['exp_num'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    exp_data_data = cumulative_avg_reward_by_exp_num.loc[cumulative_avg_reward_by_exp_num['exp_num'] == exp_num]\n",
    "    mean_cumulative_avg_reward = exp_data_data.groupby('episode')['cumulative_reward'].mean()\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Experiment {exp_num} Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Experiment Number and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot the cloud plot for each algorithm\n",
    "plt.figure(figsize=(8, 6))\n",
    "for algo in df['algorithm'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    algo_data = cumulative_avg_reward_by_algorithm.loc[cumulative_avg_reward_by_algorithm['algorithm'] == algo]\n",
    "    min_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = algo_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Algorithm {algo} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Algorithm and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 8))  # Increased size for better visibility\n",
    "for algo in cumulative_avg_reward_by_algorithm_season['algorithm'].unique():\n",
    "    for season in cumulative_avg_reward_by_algorithm_season['season'].unique():\n",
    "        # Filter the data for the current algorithm and season\n",
    "        data = cumulative_avg_reward_by_algorithm_season.loc[\n",
    "            (cumulative_avg_reward_by_algorithm_season['algorithm'] == algo) &\n",
    "            (cumulative_avg_reward_by_algorithm_season['season'] == season)\n",
    "        ]\n",
    "        \n",
    "        if data.empty:\n",
    "            continue  # Skip if no data for this combination\n",
    "        \n",
    "        # Calculate min, max, and mean cumulative rewards per episode\n",
    "        min_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].min()\n",
    "        max_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].max()\n",
    "        mean_cumulative_avg_reward = data.groupby('episode')['cumulative_avg_reward'].mean()\n",
    "        \n",
    "        # Plot the filled area between min and max\n",
    "        plt.fill_between(\n",
    "            min_cumulative_avg_reward.index, \n",
    "            min_cumulative_avg_reward.values, \n",
    "            max_cumulative_avg_reward.values, \n",
    "            alpha=0.3\n",
    "        )\n",
    "        \n",
    "        # Plot the mean cumulative average reward\n",
    "        plt.plot(\n",
    "            mean_cumulative_avg_reward.index, \n",
    "            mean_cumulative_avg_reward.values, \n",
    "            label=f'Algorithm {algo} Season {season} Average Reward'\n",
    "        )\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(\n",
    "    f'Aggs. {federated_c[\"aggregation_count\"]} - '\n",
    "    f'Eps. per agg. {nn_c[\"num_episodes\"]} - '\n",
    "    f'Cumulative Average Reward per Episode by Algorithm and Season'\n",
    ")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')  # Place legend outside the plot\n",
    "plt.grid(True)\n",
    "plt.tight_layout()  # Adjust layout to accommodate legend\n",
    "plt.show()\n",
    "\n",
    "# Plot the cloud plot for each season\n",
    "plt.figure(figsize=(8, 6))\n",
    "for season in df['season'].unique():\n",
    "    # Filter the data for the current zone\n",
    "    season_data = cumulative_avg_reward_by_season.loc[cumulative_avg_reward_by_season['season'] == season]\n",
    "    min_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].min()\n",
    "    max_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].max()\n",
    "    mean_cumulative_avg_reward = season_data.groupby('episode')['cumulative_reward'].mean()\n",
    "\n",
    "    plt.fill_between(\n",
    "        min_cumulative_avg_reward.index, \n",
    "        min_cumulative_avg_reward.values, \n",
    "        max_cumulative_avg_reward.values, \n",
    "        alpha=0.3\n",
    "    )\n",
    "    plt.plot(\n",
    "        mean_cumulative_avg_reward.index, \n",
    "        mean_cumulative_avg_reward.values, \n",
    "        label=f'Season {season} Average Reward'\n",
    "    )\n",
    "\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Average Reward')\n",
    "plt.title(f'Aggs. {federated_c[\"aggregation_count\"]} - Eps. per agg. {nn_c[\"num_episodes\"]} - Cumulative Average Reward per Episode by Season and Seed')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
