{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45773e86-ab4f-42f0-b5c2-eb9893e550ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from evaluation import *\n",
    "from data_loader import *\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f179a2-7dd0-4b64-a41b-36d53d61dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"DQN\": 'darkorange',\n",
    "    \"PPO\": 'forestgreen',\n",
    "    \"CMA\": 'cyan',\n",
    "    \"ODT\": 'blueviolet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047648a7-4749-4142-a7ee-134f4b48ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4072 does not have matching aggregation level\n",
      "Experiment 4073 does not have matching aggregation level\n",
      "Experiment 4074 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4075/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4076/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4077/train/metrics....csv\n",
      "Experiment 4078 does not have matching aggregation level\n",
      "Experiment 4079 does not have matching aggregation level\n",
      "Experiment 4080 does not have matching aggregation level\n",
      "Experiment 4081 does not have matching aggregation level\n",
      "Experiment 4082 does not have matching aggregation level\n",
      "Experiment 4083 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4084/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4085/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4086/train/metrics....csv\n",
      "Experiment 4087 does not have matching aggregation level\n",
      "Experiment 4088 does not have matching aggregation level\n",
      "Experiment 4089 does not have matching aggregation level\n",
      "Experiment 4090 does not have matching aggregation level\n",
      "Experiment 4091 does not have matching aggregation level\n",
      "Experiment 4092 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4093/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4094/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4095/train/metrics....csv\n",
      "Experiment 4096 does not have matching aggregation level\n",
      "Experiment 4097 does not have matching aggregation level\n",
      "Experiment 4098 does not have matching aggregation level\n",
      "Experiment 4099 does not have matching aggregation level\n",
      "Experiment 4100 does not have matching aggregation level\n",
      "Experiment 4101 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4102/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4103/train/metrics....csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4104/train/metrics....csv\n",
      "Experiment 4105 does not have matching aggregation level\n",
      "Experiment 4106 does not have matching aggregation level\n",
      "Experiment 4107 does not have matching aggregation level\n"
     ]
    }
   ],
   "source": [
    "starting_exp = 4072\n",
    "ending_exp = 4107\n",
    "save_processed_data = True\n",
    "load_agent = True\n",
    "load_station = False\n",
    "#exp_lists_preformatted = ['4000_4035.csv', '4072_4107.csv'] # Existing formatted datasets\n",
    "exp_lists_preformatted = []\n",
    "\n",
    "target_agg_count = 6\n",
    "\n",
    "exp_agent_data = []\n",
    "exp_station_data = []\n",
    "\n",
    "experiments = range(starting_exp, ending_exp + 1)\n",
    "\n",
    "# REMOVE THIS\n",
    "#algos = ['DQN', 'PPO', 'CMA', 'ODT']\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "    for ind, exp_num in enumerate(experiments):\n",
    "        config_fname = f'./Exp_{exp_num}/config.yaml'\n",
    "        \n",
    "        c = load_config_file(config_fname)\n",
    "        nn_c = c['nn_hyperparameters']\n",
    "        federated_c = c['federated_learning_settings']\n",
    "        algo_c = c['algorithm_settings']\n",
    "        env_c = c['environment_settings']\n",
    "        eval_c = c['eval_config']\n",
    "        cma_c = c['cma_parameters']\n",
    "    \n",
    "        if federated_c['aggregation_count'] != target_agg_count:\n",
    "            print(f\"Experiment {exp_num} does not have matching aggregation level\")\n",
    "            continue\n",
    "        \n",
    "        ev_info = []\n",
    "    \n",
    "        seed = env_c['seed']\n",
    "    \n",
    "        algorithm_dm = algo_c['algorithm']\n",
    "        \n",
    "        def load_from_json_with_error_handling(filepath, columns_specific):\n",
    "            try:\n",
    "                return read_csv_data(filepath, columns=columns_specific)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "                return None  # Handle the error and return None or an empty object\n",
    "        \n",
    "        \n",
    "        d_base = f\"../../../../storage_1/metrics/Exp_{exp_num}\"\n",
    "        \n",
    "        if not os.path.exists(d_base):\n",
    "            d_base = f\"../metrics/Exp_{exp_num}\"\n",
    "                \n",
    "        base_path = f\"{d_base}/train/metrics\"\n",
    "    \n",
    "        print(f'Loading {base_path}....csv')\n",
    "        if load_agent:\n",
    "            agent_data = load_from_json_with_error_handling(f'{base_path}_agent_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'distance', 'average_battery'])\n",
    "            agent_data['seed'] = seed\n",
    "            agent_data['exp_num'] = exp_num\n",
    "            agent_data['algorithm'] = algorithm_dm\n",
    "            #agent_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "            agent_data['season'] = env_c['season']\n",
    "            agent_data['num_aggs'] = federated_c['aggregation_count']\n",
    "            agent_data['eps_per_agg'] = cma_c['max_generations'] if algorithm_dm == 'CMA' else nn_c['num_episodes']\n",
    "            exp_agent_data.append(agent_data)\n",
    "        if load_station:\n",
    "            station_data = load_from_json_with_error_handling(f'{base_path}_station_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'station_index', 'traffic'])\n",
    "            station_data['seed'] = seed        \n",
    "            station_data['exp_num'] = exp_num     \n",
    "            station_data['algorithm'] = algorithm_dm\n",
    "            #station_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "            station_data['season'] = env_c['season']\n",
    "            station_data['num_aggs'] = federated_c['aggregation_count']\n",
    "            station_data['eps_per_agg'] = cma_c['max_generations'] if algorithm_dm == 'CMA' else nn_c['num_episodes']\n",
    "            exp_station_data.append(station_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c374987-9920-4ebe-903a-4383fff87814",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_lists_preformatted) == 0:\n",
    "    # Convert data to DataFrame for easier manipulation\n",
    "    if load_agent:\n",
    "        df_agent = pd.concat(exp_agent_data, ignore_index=True)\n",
    "\n",
    "        cumulative_agent_df = (\n",
    "            df_agent\n",
    "            .groupby(\n",
    "                ['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "            )[['distance', 'average_battery']]\n",
    "            .sum()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        # Rename the 'reward' column to 'cumulative_reward' for clarity\n",
    "        cumulative_agent_df.rename(columns={'average_battery': 'battery_used'}, inplace=True)\n",
    "        cumulative_agent_df.rename(columns={'distance': 'distance_traveled'}, inplace=True)\n",
    "        \n",
    "        cumulative_agent_df['episode'] = cumulative_agent_df['aggregation'] * cumulative_agent_df['eps_per_agg'] + cumulative_agent_df['episode']\n",
    "    \n",
    "    if load_station:\n",
    "        df_station = pd.concat(exp_station_data, ignore_index=True)\n",
    "    \n",
    "        print('Cumulating data')    \n",
    "        cumulative_station_df = (\n",
    "            df_station\n",
    "            .groupby(\n",
    "                ['episode', 'zone', 'aggregation', 'station_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "            )[['traffic']]\n",
    "            .max()\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        print('Recalculating episode')    \n",
    "        # Get recalculated episodes using (aggregation number * episodes per aggregation) + episode number\n",
    "        cumulative_station_df['episode'] = cumulative_station_df['aggregation'] * cumulative_station_df['eps_per_agg'] + cumulative_station_df['episode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0607a2a7-5aae-4016-af67-095245cdc777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to ../../../../storage_1/metrics/formatted_experiment_data/part_4//agent_4072_4107.csv\n"
     ]
    }
   ],
   "source": [
    "# Define the file path with starting_exp and ending_exp variables\n",
    "file_path_for_processed_data = f'../../../../storage_1/metrics/formatted_experiment_data/part_4/'\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "\n",
    "    # Get the data for the last episode only\n",
    "    if load_agent:\n",
    "        cumulative_agent_df = cumulative_agent_df[cumulative_agent_df['episode'] == cumulative_agent_df['episode'].max()]\n",
    "    \n",
    "    if load_station:\n",
    "        cumulative_station_df = cumulative_station_df[cumulative_station_df['episode'] == cumulative_station_df['episode'].max()]\n",
    "\n",
    "    \n",
    "    if save_processed_data:\n",
    "        if not os.path.exists(file_path_for_processed_data):\n",
    "            os.makedirs(file_path_for_processed_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        if load_agent:\n",
    "            cumulative_agent_df.to_csv(f\"{file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "            print(f\"Data successfully saved to {file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\")\n",
    "\n",
    "        if load_station:\n",
    "            cumulative_station_df.to_csv(f\"{file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "            print(f\"Data successfully saved to {file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\")\n",
    "\n",
    "if len(exp_lists_preformatted) > 0:\n",
    "    agent_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/agent_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        agent_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_agent_df = pd.concat(agent_dataframes, ignore_index=True)\n",
    "\n",
    "    station_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/station_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        station_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_station_df = pd.concat(station_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f1bac0-1dcf-4daf-b986-efb58314357a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Algorithm: CMA =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cumulative_station_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m===== Algorithm: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m agent_d \u001b[38;5;241m=\u001b[39m cumulative_agent_df[cumulative_agent_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m algo]\n\u001b[0;32m----> 5\u001b[0m station_d \u001b[38;5;241m=\u001b[39m \u001b[43mcumulative_station_df\u001b[49m[cumulative_station_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124malgorithm\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m algo]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAverage Traffic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstation_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraffic\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAverage Distance Travelled: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00magent_d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance_traveled\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cumulative_station_df' is not defined"
     ]
    }
   ],
   "source": [
    "for algo in cumulative_agent_df['algorithm'].unique():\n",
    "    print(f\"===== Algorithm: {algo} =====\")\n",
    "\n",
    "    agent_d = cumulative_agent_df[cumulative_agent_df['algorithm'] == algo]\n",
    "    station_d = cumulative_station_df[cumulative_station_df['algorithm'] == algo]\n",
    "\n",
    "    print(f\"\\tAverage Traffic: {station_d['traffic'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Distance Travelled: {agent_d['distance_traveled'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Battery Used (kWh): {agent_d['battery_used'].mean():.2f}\")\n",
    "    print(\"\\t--\")\n",
    "    print(f\"\\tTraffic Std Dev: {station_d['traffic'].std():.2f}\")\n",
    "    print(f\"\\tDistance Travelled Std Dev: {agent_d['distance_traveled'].std():.2f}\")\n",
    "    print(f\"\\tBattery Used Std Dev (kWh): {agent_d['battery_used'].std():.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for season in cumulative_agent_df['season'].unique():\n",
    "    season_data = cumulative_agent_df[cumulative_agent_df['season'] == season]\n",
    "    \n",
    "    for algo in season_data['algorithm'].unique():\n",
    "        algo_data = season_data[season_data['algorithm'] == algo]\n",
    "\n",
    "        avg_energy_used = algo_data['battery_used'].mean()\n",
    "        std_energy_used = algo_data['battery_used'].std()\n",
    "\n",
    "        print(f\"Algorotim: {algo} - Season: {season} \\\n",
    "        - Avg. Energy {avg_energy_used:.2f} - Std. Energy {std_energy_used:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
