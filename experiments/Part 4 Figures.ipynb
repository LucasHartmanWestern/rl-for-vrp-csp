{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45773e86-ab4f-42f0-b5c2-eb9893e550ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from evaluation import *\n",
    "from data_loader import *\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f179a2-7dd0-4b64-a41b-36d53d61dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"DQN\": 'darkorange',\n",
    "    \"PPO\": 'forestgreen',\n",
    "    \"CMA\": 'cyan',\n",
    "    \"ODT\": 'blueviolet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047648a7-4749-4142-a7ee-134f4b48ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4000 does not have matching aggregation level\n",
      "Experiment 4001 does not have matching aggregation level\n",
      "Experiment 4002 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4003/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4004/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4005/train/metrics_agent_metrics.csv\n",
      "Experiment 4006 does not have matching aggregation level\n",
      "Experiment 4007 does not have matching aggregation level\n",
      "Experiment 4008 does not have matching aggregation level\n",
      "Experiment 4009 does not have matching aggregation level\n",
      "Experiment 4010 does not have matching aggregation level\n",
      "Experiment 4011 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4012/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4013/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4014/train/metrics_agent_metrics.csv\n",
      "Experiment 4015 does not have matching aggregation level\n",
      "Experiment 4016 does not have matching aggregation level\n",
      "Experiment 4017 does not have matching aggregation level\n",
      "Experiment 4018 does not have matching aggregation level\n",
      "Experiment 4019 does not have matching aggregation level\n",
      "Experiment 4020 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4021/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4022/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4023/train/metrics_agent_metrics.csv\n",
      "Experiment 4024 does not have matching aggregation level\n",
      "Experiment 4025 does not have matching aggregation level\n",
      "Experiment 4026 does not have matching aggregation level\n",
      "Experiment 4027 does not have matching aggregation level\n",
      "Experiment 4028 does not have matching aggregation level\n",
      "Experiment 4029 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4030/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4031/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4032/train/metrics_agent_metrics.csv\n",
      "Experiment 4033 does not have matching aggregation level\n",
      "Experiment 4034 does not have matching aggregation level\n",
      "Experiment 4035 does not have matching aggregation level\n"
     ]
    }
   ],
   "source": [
    "starting_exp = 4000\n",
    "ending_exp = 4035\n",
    "save_processed_data = True\n",
    "#exp_lists_preformatted = ['4000_4035.csv', '4072_4107.csv'] # Existing formatted datasets\n",
    "exp_lists_preformatted = []\n",
    "\n",
    "target_agg_count = 6\n",
    "\n",
    "exp_agent_data = []\n",
    "exp_station_data = []\n",
    "\n",
    "experiments = range(starting_exp, ending_exp + 1)\n",
    "\n",
    "# REMOVE THIS\n",
    "#algos = ['DQN', 'PPO', 'CMA', 'ODT']\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "    for ind, exp_num in enumerate(experiments):\n",
    "        config_fname = f'./Exp_{exp_num}/config.yaml'\n",
    "        \n",
    "        c = load_config_file(config_fname)\n",
    "        nn_c = c['nn_hyperparameters']\n",
    "        federated_c = c['federated_learning_settings']\n",
    "        algo_c = c['algorithm_settings']\n",
    "        env_c = c['environment_settings']\n",
    "        eval_c = c['eval_config']\n",
    "    \n",
    "        if federated_c['aggregation_count'] != target_agg_count:\n",
    "            print(f\"Experiment {exp_num} does not have matching aggregation level\")\n",
    "            continue\n",
    "        \n",
    "        ev_info = []\n",
    "    \n",
    "        seed = env_c['seed']\n",
    "    \n",
    "        algorithm_dm = algo_c['algorithm']\n",
    "        \n",
    "        def load_from_json_with_error_handling(filepath, columns_specific):\n",
    "            try:\n",
    "                return read_csv_data(filepath, columns=columns_specific)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "                return None  # Handle the error and return None or an empty object\n",
    "        \n",
    "        \n",
    "        d_base = f\"../../../../storage_1/metrics/Exp_{exp_num}\"\n",
    "        \n",
    "        if not os.path.exists(d_base):\n",
    "            d_base = f\"../metrics/Exp_{exp_num}\"\n",
    "                \n",
    "        base_path = f\"{d_base}/train/metrics\"\n",
    "    \n",
    "        print(f'Loading {base_path}_agent_metrics.csv')\n",
    "        agent_data = load_from_json_with_error_handling(f'{base_path}_agent_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'distance', 'average_battery'])\n",
    "        station_data = load_from_json_with_error_handling(f'{base_path}_station_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'station_index', 'traffic'])\n",
    "        \n",
    "        agent_data['seed'] = seed\n",
    "        station_data['seed'] = seed\n",
    "        \n",
    "        agent_data['exp_num'] = exp_num\n",
    "        station_data['exp_num'] = exp_num\n",
    "        \n",
    "        agent_data['algorithm'] = algorithm_dm\n",
    "        station_data['algorithm'] = algorithm_dm\n",
    "        #agent_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "        #station_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "        \n",
    "        agent_data['season'] = env_c['season']\n",
    "        station_data['season'] = env_c['season']\n",
    "        \n",
    "        agent_data['num_aggs'] = federated_c['aggregation_count']\n",
    "        station_data['num_aggs'] = federated_c['aggregation_count']\n",
    "        \n",
    "        agent_data['eps_per_agg'] = nn_c['num_episodes']\n",
    "        station_data['eps_per_agg'] = nn_c['num_episodes']\n",
    "    \n",
    "        exp_agent_data.append(agent_data)\n",
    "        exp_station_data.append(station_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c374987-9920-4ebe-903a-4383fff87814",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_lists_preformatted) == 0:\n",
    "    # Convert data to DataFrame for easier manipulation\n",
    "    df_agent = pd.concat(exp_agent_data, ignore_index=True)\n",
    "    df_station = pd.concat(exp_station_data, ignore_index=True)\n",
    "    \n",
    "    cumulative_agent_df = (\n",
    "        df_agent\n",
    "        .groupby(\n",
    "            ['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "        )[['distance', 'average_battery']]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    cumulative_station_df = (\n",
    "        df_station\n",
    "        .groupby(\n",
    "            ['episode', 'zone', 'aggregation', 'station_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "        )['traffic']\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename the 'reward' column to 'cumulative_reward' for clarity\n",
    "    cumulative_agent_df.rename(columns={'average_battery': 'battery_used'}, inplace=True)\n",
    "    cumulative_agent_df.rename(columns={'distance': 'distance_traveled'}, inplace=True)\n",
    "    \n",
    "    # Get recalculated episodes using (aggregation number * episodes per aggregation) + episode number\n",
    "    cumulative_agent_df['episode'] = cumulative_agent_df['aggregation'] * cumulative_agent_df['eps_per_agg'] + cumulative_agent_df['episode']\n",
    "    cumulative_station_df['episode'] = cumulative_station_df['aggregation'] * cumulative_station_df['eps_per_agg'] + cumulative_station_df['episode']\n",
    "    \n",
    "    cumulative_agent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9f8b6-41d9-413e-b51e-2587654c13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_lists_preformatted) == 0:\n",
    "    cumulative_station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607a2a7-5aae-4016-af67-095245cdc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path with starting_exp and ending_exp variables\n",
    "file_path_for_processed_data = f'../../../../storage_1/metrics/formatted_experiment_data/part_4/'\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "\n",
    "    # Get the data for the last episode only\n",
    "    cumulative_agent_df = cumulative_agent_df[cumulative_agent_df['episode'] == cumulative_agent_df['episode'].max()]\n",
    "    cumulative_station_df = cumulative_station_df[cumulative_station_df['episode'] == cumulative_station_df['episode'].max()]\n",
    "\n",
    "    if save_processed_data:\n",
    "        if not os.path.exists(file_path_for_processed_data):\n",
    "            os.makedirs(file_path_for_processed_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        cumulative_agent_df.to_csv(f\"{file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "        cumulative_station_df.to_csv(f\"{file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "        \n",
    "        print(f\"Data successfully saved to {file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\")\n",
    "        print(f\"Data successfully saved to {file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\")\n",
    "\n",
    "if len(exp_lists_preformatted) > 0:\n",
    "    agent_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/agent_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        agent_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_agent_df = pd.concat(agent_dataframes, ignore_index=True)\n",
    "\n",
    "    station_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/station_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        station_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_station_df = pd.concat(station_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1bac0-1dcf-4daf-b986-efb58314357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo in cumulative_agent_df['algorithm'].unique():\n",
    "    print(f\"===== Algorithm: {algo} =====\")\n",
    "\n",
    "    agent_d = cumulative_agent_df[cumulative_agent_df['algorithm'] == algo]\n",
    "    station_d = cumulative_station_df[cumulative_station_df['algorithm'] == algo]\n",
    "\n",
    "    print(f\"\\tAverage Traffic: {station_d['traffic'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Distance Travelled: {agent_d['distance_traveled'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Battery Used (kWh): {agent_d['battery_used'].mean():.2f}\")\n",
    "    print(\"\\t--\")\n",
    "    print(f\"\\tTraffic Std Dev: {station_d['traffic'].std():.2f}\")\n",
    "    print(f\"\\tDistance Travelled Std Dev: {agent_d['distance_traveled'].std():.2f}\")\n",
    "    print(f\"\\tBattery Used Std Dev (kWh): {agent_d['battery_used'].std():.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for season in cumulative_agent_df['season'].unique():\n",
    "    season_data = cumulative_agent_df[cumulative_agent_df['season'] == season]\n",
    "    \n",
    "    for algo in season_data['algorithm'].unique():\n",
    "        algo_data = season_data[season_data['algorithm'] == algo]\n",
    "\n",
    "        avg_energy_used = algo_data['battery_used'].mean()\n",
    "        std_energy_used = algo_data['battery_used'].std()\n",
    "\n",
    "        print(f\"Algorotim: {algo} - Season: {season} \\\n",
    "        - Avg. Energy {avg_energy_used:.2f} - Std. Energy {std_energy_used:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
