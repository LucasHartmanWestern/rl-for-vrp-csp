{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45773e86-ab4f-42f0-b5c2-eb9893e550ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "from evaluation import *\n",
    "from data_loader import *\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f179a2-7dd0-4b64-a41b-36d53d61dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = {\n",
    "    \"DQN\": 'darkorange',\n",
    "    \"PPO\": 'forestgreen',\n",
    "    \"CMA\": 'cyan',\n",
    "    \"ODT\": 'blueviolet'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "047648a7-4749-4142-a7ee-134f4b48ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment 4000 does not have matching aggregation level\n",
      "Experiment 4001 does not have matching aggregation level\n",
      "Experiment 4002 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4003/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4004/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4005/train/metrics_agent_metrics.csv\n",
      "Experiment 4006 does not have matching aggregation level\n",
      "Experiment 4007 does not have matching aggregation level\n",
      "Experiment 4008 does not have matching aggregation level\n",
      "Experiment 4009 does not have matching aggregation level\n",
      "Experiment 4010 does not have matching aggregation level\n",
      "Experiment 4011 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4012/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4013/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4014/train/metrics_agent_metrics.csv\n",
      "Experiment 4015 does not have matching aggregation level\n",
      "Experiment 4016 does not have matching aggregation level\n",
      "Experiment 4017 does not have matching aggregation level\n",
      "Experiment 4018 does not have matching aggregation level\n",
      "Experiment 4019 does not have matching aggregation level\n",
      "Experiment 4020 does not have matching aggregation level\n",
      "Loading ../../../../storage_1/metrics/Exp_4021/train/metrics_agent_metrics.csv\n",
      "Loading ../../../../storage_1/metrics/Exp_4022/train/metrics_agent_metrics.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 55\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent_metrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m agent_data \u001b[38;5;241m=\u001b[39m load_from_json_with_error_handling(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent_metrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepisode\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maggregation\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maverage_battery\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m---> 55\u001b[0m station_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_json_with_error_handling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_station_metrics.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maggregation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstation_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraffic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m agent_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[1;32m     58\u001b[0m station_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n",
      "Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mload_from_json_with_error_handling\u001b[0;34m(filepath, columns_specific)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json_with_error_handling\u001b[39m(filepath, columns_specific):\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_csv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_specific\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     42\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError decoding JSON from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mcolno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/rl-for-vrp-csp(git)/data_loader.py:254\u001b[0m, in \u001b[0;36mread_csv_data\u001b[0;34m(filename, columns)\u001b[0m\n\u001b[1;32m    252\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(filename, converters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mstr\u001b[39m})\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 254\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# Parse the 'path' column using ast.literal_eval\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parsers/readers.py:1721\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1718\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1719\u001b[0m         new_rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index)\n\u001b[0;32m-> 1721\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_currow \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m new_rows\n\u001b[1;32m   1724\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py:709\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    703\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    704\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    705\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    708\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 709\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:481\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    477\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    479\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:153\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    150\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:2142\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[1;32m   2140\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[1;32m   2141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[0;32m-> 2142\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:1829\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1823\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1824\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[1;32m   1826\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[1;32m   1827\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[1;32m   1828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m-> 1829\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1830\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1831\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:2272\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2270\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2271\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2272\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2273\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2274\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2275\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2276\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/internals/managers.py:2305\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2303\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[1;32m   2304\u001b[0m new_values \u001b[38;5;241m=\u001b[39m new_values[argsort]\n\u001b[0;32m-> 2305\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m \u001b[43mnew_mgr_locs\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2307\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [new_block_2d(new_values, placement\u001b[38;5;241m=\u001b[39mbp)], \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "starting_exp = 4000\n",
    "ending_exp = 4035\n",
    "save_processed_data = True\n",
    "#exp_lists_preformatted = ['4000_4035.csv', '4072_4107.csv'] # Existing formatted datasets\n",
    "exp_lists_preformatted = []\n",
    "\n",
    "target_agg_count = 6\n",
    "\n",
    "exp_agent_data = []\n",
    "exp_station_data = []\n",
    "\n",
    "experiments = range(starting_exp, ending_exp + 1)\n",
    "\n",
    "# REMOVE THIS\n",
    "#algos = ['DQN', 'PPO', 'CMA', 'ODT']\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "    for ind, exp_num in enumerate(experiments):\n",
    "        config_fname = f'./Exp_{exp_num}/config.yaml'\n",
    "        \n",
    "        c = load_config_file(config_fname)\n",
    "        nn_c = c['nn_hyperparameters']\n",
    "        federated_c = c['federated_learning_settings']\n",
    "        algo_c = c['algorithm_settings']\n",
    "        env_c = c['environment_settings']\n",
    "        eval_c = c['eval_config']\n",
    "    \n",
    "        if federated_c['aggregation_count'] != target_agg_count:\n",
    "            print(f\"Experiment {exp_num} does not have matching aggregation level\")\n",
    "            continue\n",
    "        \n",
    "        ev_info = []\n",
    "    \n",
    "        seed = env_c['seed']\n",
    "    \n",
    "        algorithm_dm = algo_c['algorithm']\n",
    "        \n",
    "        def load_from_json_with_error_handling(filepath, columns_specific):\n",
    "            try:\n",
    "                return read_csv_data(filepath, columns=columns_specific)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "                return None  # Handle the error and return None or an empty object\n",
    "        \n",
    "        \n",
    "        d_base = f\"../../../../storage_1/metrics/Exp_{exp_num}\"\n",
    "        \n",
    "        if not os.path.exists(d_base):\n",
    "            d_base = f\"../metrics/Exp_{exp_num}\"\n",
    "                \n",
    "        base_path = f\"{d_base}/train/metrics\"\n",
    "    \n",
    "        print(f'Loading {base_path}_agent_metrics.csv')\n",
    "        agent_data = load_from_json_with_error_handling(f'{base_path}_agent_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'distance', 'average_battery'])\n",
    "        station_data = load_from_json_with_error_handling(f'{base_path}_station_metrics.csv', ['episode', 'timestep', 'done', 'zone', 'aggregation', 'station_index', 'traffic'])\n",
    "        \n",
    "        agent_data['seed'] = seed\n",
    "        station_data['seed'] = seed\n",
    "        \n",
    "        agent_data['exp_num'] = exp_num\n",
    "        station_data['exp_num'] = exp_num\n",
    "        \n",
    "        agent_data['algorithm'] = algorithm_dm\n",
    "        station_data['algorithm'] = algorithm_dm\n",
    "        #agent_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "        #station_data['algorithm'] = algos[ind % len(algos)] # REMOVE THIS AND UNCOMMENT PREVIOUS LINE!\n",
    "        \n",
    "        agent_data['season'] = env_c['season']\n",
    "        station_data['season'] = env_c['season']\n",
    "        \n",
    "        agent_data['num_aggs'] = federated_c['aggregation_count']\n",
    "        station_data['num_aggs'] = federated_c['aggregation_count']\n",
    "        \n",
    "        agent_data['eps_per_agg'] = nn_c['num_episodes']\n",
    "        station_data['eps_per_agg'] = nn_c['num_episodes']\n",
    "    \n",
    "        exp_agent_data.append(agent_data)\n",
    "        exp_station_data.append(station_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c374987-9920-4ebe-903a-4383fff87814",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_lists_preformatted) == 0:\n",
    "    # Convert data to DataFrame for easier manipulation\n",
    "    df_agent = pd.concat(exp_agent_data, ignore_index=True)\n",
    "    df_station = pd.concat(exp_station_data, ignore_index=True)\n",
    "    \n",
    "    cumulative_agent_df = (\n",
    "        df_agent\n",
    "        .groupby(\n",
    "            ['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "        )[['distance', 'average_battery']]\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    cumulative_station_df = (\n",
    "        df_station\n",
    "        .groupby(\n",
    "            ['episode', 'zone', 'aggregation', 'station_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "        )['traffic']\n",
    "        .max()\n",
    "        .reset_index()\n",
    "    )\n",
    "    \n",
    "    # Rename the 'reward' column to 'cumulative_reward' for clarity\n",
    "    cumulative_agent_df.rename(columns={'average_battery': 'battery_used'}, inplace=True)\n",
    "    cumulative_agent_df.rename(columns={'distance': 'distance_traveled'}, inplace=True)\n",
    "    \n",
    "    # Get recalculated episodes using (aggregation number * episodes per aggregation) + episode number\n",
    "    cumulative_agent_df['episode'] = cumulative_agent_df['aggregation'] * cumulative_agent_df['eps_per_agg'] + cumulative_agent_df['episode']\n",
    "    cumulative_station_df['episode'] = cumulative_station_df['aggregation'] * cumulative_station_df['eps_per_agg'] + cumulative_station_df['episode']\n",
    "    \n",
    "    cumulative_agent_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b9f8b6-41d9-413e-b51e-2587654c13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(exp_lists_preformatted) == 0:\n",
    "    cumulative_station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0607a2a7-5aae-4016-af67-095245cdc777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path with starting_exp and ending_exp variables\n",
    "file_path_for_processed_data = f'../../../../storage_1/metrics/formatted_experiment_data/part_4/'\n",
    "\n",
    "if len(exp_lists_preformatted) == 0:\n",
    "\n",
    "    # Get the data for the last episode only\n",
    "    cumulative_agent_df = cumulative_agent_df[cumulative_agent_df['episode'] == cumulative_agent_df['episode'].max()]\n",
    "    cumulative_station_df = cumulative_station_df[cumulative_station_df['episode'] == cumulative_station_df['episode'].max()]\n",
    "\n",
    "    if save_processed_data:\n",
    "        if not os.path.exists(file_path_for_processed_data):\n",
    "            os.makedirs(file_path_for_processed_data)\n",
    "        \n",
    "        # Save the DataFrame to a CSV file\n",
    "        cumulative_agent_df.to_csv(f\"{file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "        cumulative_station_df.to_csv(f\"{file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\", index=False)\n",
    "        \n",
    "        print(f\"Data successfully saved to {file_path_for_processed_data}/agent_{starting_exp}_{ending_exp}.csv\")\n",
    "        print(f\"Data successfully saved to {file_path_for_processed_data}/station_{starting_exp}_{ending_exp}.csv\")\n",
    "\n",
    "if len(exp_lists_preformatted) > 0:\n",
    "    agent_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/agent_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        agent_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_agent_df = pd.concat(agent_dataframes, ignore_index=True)\n",
    "\n",
    "    station_dataframes = []\n",
    "    \n",
    "    # Load and combine datasets\n",
    "    for existing_set in exp_lists_preformatted:\n",
    "        loaded_dataset = pd.read_csv(f\"{file_path_for_processed_data}/station_{existing_set}\")\n",
    "        # Combine datasets\n",
    "        station_dataframes.append(loaded_dataset)\n",
    "\n",
    "    cumulative_station_df = pd.concat(station_dataframes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1bac0-1dcf-4daf-b986-efb58314357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for algo in cumulative_agent_df['algorithm'].unique():\n",
    "    print(f\"===== Algorithm: {algo} =====\")\n",
    "\n",
    "    agent_d = cumulative_agent_df[cumulative_agent_df['algorithm'] == algo]\n",
    "    station_d = cumulative_station_df[cumulative_station_df['algorithm'] == algo]\n",
    "\n",
    "    print(f\"\\tAverage Traffic: {station_d['traffic'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Distance Travelled: {agent_d['distance_traveled'].mean():.2f}\")\n",
    "    print(f\"\\tAverage Battery Used (kWh): {agent_d['battery_used'].mean():.2f}\")\n",
    "    print(\"\\t--\")\n",
    "    print(f\"\\tTraffic Std Dev: {station_d['traffic'].std():.2f}\")\n",
    "    print(f\"\\tDistance Travelled Std Dev: {agent_d['distance_traveled'].std():.2f}\")\n",
    "    print(f\"\\tBattery Used Std Dev (kWh): {agent_d['battery_used'].std():.2f}\")\n",
    "    print(\"\")\n",
    "\n",
    "for season in cumulative_agent_df['season'].unique():\n",
    "    season_data = cumulative_agent_df[cumulative_agent_df['season'] == season]\n",
    "    \n",
    "    for algo in season_data['algorithm'].unique():\n",
    "        algo_data = season_data[season_data['algorithm'] == algo]\n",
    "\n",
    "        avg_energy_used = algo_data['battery_used'].mean()\n",
    "        std_energy_used = algo_data['battery_used'].std()\n",
    "\n",
    "        print(f\"Algorotim: {algo} - Season: {season} \\\n",
    "        - Avg. Energy {avg_energy_used:.2f} - Std. Energy {std_energy_used:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
