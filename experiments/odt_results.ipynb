{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e41dd8b7-fe9f-4943-aa1f-46dfa901fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "import yaml  # If not installed: pip install pyyaml\n",
    "\n",
    "# Set up algorithm colors\n",
    "colors = {\n",
    "    \"DQN\": 'darkorange',\n",
    "    \"PPO\": 'forestgreen',\n",
    "    \"CMA\": 'cyan',\n",
    "    \"ODT\": 'blueviolet'\n",
    "}\n",
    "\n",
    "# Helper functions\n",
    "def load_config_file(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return yaml.safe_load(f)\n",
    "\n",
    "def read_csv_data(filepath, columns):\n",
    "    return pd.read_csv(filepath, usecols=columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd181982-1649-486f-859a-ee439d7d645a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading /storage_1/epigou_storage/Exp_4145/train/metrics_agent_metrics.csv for experiment 4145\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/storage_1/epigou_storage/Exp_4145/train/metrics_agent_metrics.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 46\u001b[0m\n\u001b[1;32m     44\u001b[0m base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train/metrics\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_agent_metrics.csv for experiment \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m agent_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_json_with_error_handling\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbase_path\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_agent_metrics.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mepisode\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestep\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mzone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maggregation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent_index\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreward\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m agent_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m, in \u001b[0;36mload_from_json_with_error_handling\u001b[0;34m(filepath, columns_specific)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json_with_error_handling\u001b[39m(filepath, columns_specific):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_csv_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns_specific\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m json\u001b[38;5;241m.\u001b[39mJSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError decoding JSON from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mlineno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;241m.\u001b[39mcolno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 24\u001b[0m, in \u001b[0;36mread_csv_data\u001b[0;34m(filepath, columns)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv_data\u001b[39m(filepath, columns):\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/storage_1/epigou_storage/Exp_4145/train/metrics_agent_metrics.csv'"
     ]
    }
   ],
   "source": [
    "# Define experiment range and chunk size\n",
    "starting_exp = 4145\n",
    "ending_exp = 4147\n",
    "chunk_size = 3\n",
    "\n",
    "all_experiments = list(range(starting_exp, ending_exp + 1))\n",
    "experiment_chunks = [all_experiments[i:i + chunk_size] for i in range(0, len(all_experiments), chunk_size)]\n",
    "save_processed_data = True\n",
    "file_path_for_processed_data = './processed_data'\n",
    "\n",
    "for chunk in experiment_chunks:\n",
    "    exp_agent_data = []\n",
    "    \n",
    "    for exp_num in chunk:\n",
    "        config_fname = f'./Exp_{exp_num}/config.yaml'\n",
    "        c = load_config_file(config_fname)\n",
    "        nn_c = c['nn_hyperparameters']\n",
    "        federated_c = c['federated_learning_settings']\n",
    "        algo_c = c['algorithm_settings']\n",
    "        env_c = c['environment_settings']\n",
    "        cma_c = c['cma_parameters']\n",
    "    \n",
    "        seed = env_c['seed']\n",
    "        algorithm_dm = algo_c['algorithm']\n",
    "\n",
    "        def load_from_json_with_error_handling(filepath, columns_specific):\n",
    "            try:\n",
    "                return read_csv_data(filepath, columns=columns_specific)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error decoding JSON from {filepath}: {e.msg} at line {e.lineno}, column {e.colno}\")\n",
    "                return None\n",
    "    \n",
    "        path_list = [\n",
    "            f\"/storage_1/epigou_storage/Exp_{exp_num}\",\n",
    "            f\"/mnt/storage_1/merl/metrics/Exp_{exp_num}\",\n",
    "            f\"../metrics/Exp_{exp_num}\"\n",
    "        ]\n",
    "    \n",
    "        d_base = next((p for p in path_list if os.path.exists(p)), None)\n",
    "        if d_base is None:\n",
    "            print(f\"Data folder not found for experiment {exp_num}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        base_path = f\"{d_base}/train/metrics\"\n",
    "        print(f'Loading {base_path}_agent_metrics.csv for experiment {exp_num}')\n",
    "        agent_data = load_from_json_with_error_handling(\n",
    "            f'{base_path}_agent_metrics.csv',\n",
    "            ['episode', 'timestep', 'done', 'zone', 'aggregation', 'agent_index', 'reward']\n",
    "        )\n",
    "        if agent_data is None:\n",
    "            continue\n",
    "\n",
    "        agent_data['seed'] = seed\n",
    "        agent_data['exp_num'] = exp_num\n",
    "        agent_data['algorithm'] = algorithm_dm\n",
    "        agent_data['season'] = env_c['season']\n",
    "        agent_data['num_aggs'] = federated_c['aggregation_count']\n",
    "        agent_data['eps_per_agg'] = (\n",
    "            cma_c['max_generations'] if algorithm_dm == 'CMA' else nn_c['num_episodes']\n",
    "        )\n",
    "\n",
    "        exp_agent_data.append(agent_data)\n",
    "\n",
    "    if len(exp_agent_data) == 0:\n",
    "        continue\n",
    "\n",
    "    df_agent = pd.concat(exp_agent_data, ignore_index=True)\n",
    "\n",
    "    cumulative_agent_df = (\n",
    "        df_agent\n",
    "        .groupby(\n",
    "            ['episode', 'zone', 'aggregation', 'agent_index', 'seed', 'exp_num', 'algorithm', 'season', 'num_aggs', 'eps_per_agg']\n",
    "        )['reward']\n",
    "        .sum()\n",
    "        .reset_index()\n",
    "    )\n",
    "    cumulative_agent_df.rename(columns={'reward': 'cumulative_reward'}, inplace=True)\n",
    "    cumulative_agent_df['episode'] = (\n",
    "        cumulative_agent_df['aggregation'] * cumulative_agent_df['eps_per_agg'] + cumulative_agent_df['episode']\n",
    "    )\n",
    "\n",
    "    cumulative_avg_reward_by_algorithm = (\n",
    "        cumulative_agent_df\n",
    "        .groupby(['episode', 'algorithm', 'seed', 'num_aggs', 'season', 'eps_per_agg', 'exp_num'])['cumulative_reward']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    cumulative_avg_reward_by_algorithm = cumulative_avg_reward_by_algorithm.sort_values(\n",
    "        ['season', 'algorithm', 'seed', 'num_aggs', 'episode']\n",
    "    )\n",
    "    cumulative_avg_reward_by_algorithm['cumulative_reward'] = (\n",
    "        cumulative_avg_reward_by_algorithm\n",
    "        .groupby(['algorithm', 'seed', 'num_aggs', 'season'])['cumulative_reward']\n",
    "        .transform(lambda x: x.expanding().mean())\n",
    "    )\n",
    "\n",
    "    if save_processed_data:\n",
    "        chunk_folder = os.path.join(file_path_for_processed_data, f\"{chunk[0]}_{chunk[-1]}\")\n",
    "        os.makedirs(chunk_folder, exist_ok=True)\n",
    "        csv_path = os.path.join(chunk_folder, f\"{chunk[0]}_{chunk[-1]}.csv\")\n",
    "        cumulative_avg_reward_by_algorithm.to_csv(csv_path, index=False)\n",
    "        print(f\"Data successfully saved to {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "125150cd-4a04-4526-b22a-575c26812abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing file for chunk 4145–4147; skipping plot.\n"
     ]
    }
   ],
   "source": [
    "for chunk in experiment_chunks:\n",
    "    chunk_file = os.path.join(file_path_for_processed_data, f\"{chunk[0]}_{chunk[-1]}\", f\"{chunk[0]}_{chunk[-1]}.csv\")\n",
    "    if not os.path.exists(chunk_file):\n",
    "        print(f\"Missing file for chunk {chunk[0]}–{chunk[-1]}; skipping plot.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(chunk_file)\n",
    "\n",
    "    # Check for mixed seasons (fall back to 'Mixed' if more than one unique value)\n",
    "    unique_seasons = df['season'].unique()\n",
    "    season_label = unique_seasons[0] if len(unique_seasons) == 1 else \"Mixed\"\n",
    "\n",
    "    agg_data = (\n",
    "        df\n",
    "        .groupby(['episode', 'algorithm'])['cumulative_reward']\n",
    "        .agg(['mean', 'min', 'max'])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    for algo in agg_data['algorithm'].unique():\n",
    "        algo_data = agg_data[agg_data['algorithm'] == algo]\n",
    "        ax.fill_between(\n",
    "            algo_data['episode'],\n",
    "            algo_data['min'],\n",
    "            algo_data['max'],\n",
    "            color=colors.get(algo, 'gray'),\n",
    "            alpha=0.3\n",
    "        )\n",
    "        ax.plot(\n",
    "            algo_data['episode'],\n",
    "            algo_data['mean'],\n",
    "            color=colors.get(algo, 'gray'),\n",
    "            label=f'{algo} Average Reward'\n",
    "        )\n",
    "\n",
    "    # Estimate eps_per_agg if needed\n",
    "    if 'eps_per_agg' in df.columns:\n",
    "        eps_per_agg = df['eps_per_agg'].iloc[0]\n",
    "    else:\n",
    "        num_aggs = df['num_aggs'].iloc[0]\n",
    "        max_episode = df['episode'].max()\n",
    "        eps_per_agg = int(np.ceil(max_episode / num_aggs))\n",
    "\n",
    "    num_aggs = df['num_aggs'].iloc[0]\n",
    "    for agg in range(1, int(num_aggs) + 1):\n",
    "        ax.axvline(x=agg * eps_per_agg, color='r', linestyle='--', linewidth=1, alpha=0.7)\n",
    "\n",
    "    # Add season label in top-left corner\n",
    "    x_pos = df['episode'].min()\n",
    "    y_pos = df['cumulative_reward'].max() * 0.95\n",
    "    ax.text(x_pos + 20, y_pos, f\"Season: {season_label}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Average Reward')\n",
    "    ax.set_title(f'Experiments {chunk[0]}–{chunk[-1]}')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(0, 5000)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0afecb7-63d3-48c1-87b0-2d51ad61da10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing file for chunk 4145–4147; skipping.\n"
     ]
    }
   ],
   "source": [
    "for chunk in experiment_chunks:\n",
    "    chunk_file = os.path.join(file_path_for_processed_data, f\"{chunk[0]}_{chunk[-1]}\", f\"{chunk[0]}_{chunk[-1]}.csv\")\n",
    "    if not os.path.exists(chunk_file):\n",
    "        print(f\"Missing file for chunk {chunk[0]}–{chunk[-1]}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_chunk = pd.read_csv(chunk_file)\n",
    "\n",
    "    for exp_num in df_chunk['exp_num'].unique():\n",
    "        df = df_chunk[df_chunk['exp_num'] == exp_num]\n",
    "        unique_seasons = df['season'].unique()\n",
    "        season_label = unique_seasons[0] if len(unique_seasons) == 1 else \"Mixed\"\n",
    "\n",
    "        agg_data = (\n",
    "            df\n",
    "            .groupby(['episode', 'algorithm'])['cumulative_reward']\n",
    "            .agg(['mean', 'min', 'max'])\n",
    "            .reset_index()\n",
    "        )\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "        for algo in agg_data['algorithm'].unique():\n",
    "            algo_data = agg_data[agg_data['algorithm'] == algo]\n",
    "            ax.fill_between(algo_data['episode'], algo_data['min'], algo_data['max'], color=colors.get(algo, 'gray'), alpha=0.3)\n",
    "            ax.plot(algo_data['episode'], algo_data['mean'], color=colors.get(algo, 'gray'), label=f'{algo} Average Reward')\n",
    "\n",
    "        eps_per_agg = df['eps_per_agg'].iloc[0] if 'eps_per_agg' in df.columns else int(np.ceil(df['episode'].max() / df['num_aggs'].iloc[0]))\n",
    "        for agg in range(1, int(df['num_aggs'].iloc[0]) + 1):\n",
    "            ax.axvline(x=agg * eps_per_agg, color='r', linestyle='--', linewidth=1, alpha=0.7)\n",
    "\n",
    "        ax.text(20, df['cumulative_reward'].max() * 0.95, f\"Season: {season_label}\", fontsize=12, bbox=dict(facecolor='white', alpha=0.5))\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Cumulative Average Reward')\n",
    "        ax.set_title(f'Experiment {exp_num}: Mean + Min/Max Across Seeds')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        ax.set_xlim(0, 5000)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398ade1f-c38f-478d-a74e-f8d585bff8ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
